---
title: 'èª­å”‡è¡“ã®ç ”ç©¶ã¾ã¨ã‚ã€ã‚µãƒ¼ãƒ™ã‚¤ã€‘'
emoji: "ğŸ’„"
type: "tech"
topics: ["vsr", "machinelearning", "deeplearning", "LLM"]
published: false
references:
- OpenASR Leaderboard: https://chatgpt.com/c/68faf914-d57c-8322-ac80-2f178b8eae6e?ref=mini
- https://chatgpt.com/c/68fb1d9c-1540-8322-9307-4bef509a67f8?ref=mini
- RAVEn/BRAVEn: https://chatgpt.com/c/68fb298e-7ccc-8320-a573-c31bf48e2bad?ref=mini
- å¼±æ•™å¸«ã‚ã‚Šå­¦ç¿’ãªã©: https://chatgpt.com/c/68fd58f1-24b8-8323-bdd8-5832a50ae0ce?ref=mini
- LP-Conformer: https://chatgpt.com/c/68fd5331-2e30-8320-bd0d-817e1a448489?ref=mini
---

## TL;DR



## å‹•æ©Ÿ

èª­å”‡è¡“ã®ãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºã™ã‚‹ãŸã‚ã«ã¯ã€è¦–è¦šéŸ³å£°èªè­˜(VSR)ã ã‘ã§ãªãéŸ³å£°èªè­˜(ASR)ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æŠ‘ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãã“ã§ã€ASR/VSRã«ãŠã‘ã‚‹ä»£è¡¨çš„ãªãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»å­¦ç¿’æ‰‹æ³•ãªã©ã«ã¤ã„ã¦ã¾ã¨ã‚ã¾ã—ãŸã€‚

æ™‚é–“ã‚’ã‹ã‘ã¦æ³¨æ„æ·±ãèª¿ã¹ã¦ã„ã¾ã™ãŒã€ç­†è€…ã¯å°‚é–€çš„ãªè¨“ç·´ã‚’å—ã‘ã¦ã„ãªã„ãŸã‚ã€æœ¬è¨˜äº‹ã«ã¯èª¤ã‚ŠãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

## VSRã¨ã¯

VSR(Visual Speech Recognition)ã¨ã¯ã€è¦–è¦šãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç™ºè©±å†…å®¹ã‚’èªè­˜ã™ã‚‹ã€ã„ã‚ã°èª­å”‡è¡“ã®ã‚¿ã‚¹ã‚¯ã§ã™ã€‚ãã‚Œä»¥å¤–ã«A-VSR(Auto VSR)ã‚„Lip Readingã‚„Silent Speech Recognitionã¨å‘¼ã°ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚

é–¢é€£ã™ã‚‹ã‚¿ã‚¹ã‚¯ã¨ã—ã¦ã¯ã€éŸ³å£°ã‹ã‚‰ã®æ–‡å­—èµ·ã“ã—ã‚’è¡Œã†ASR(Audio Visual Recognition)ã‚„ã€è¦–è¦šã‚’ç”¨ã„ã¦ãƒã‚¤ã‚ºãªã©ã®å½±éŸ¿ã‚’å–ã‚Šé™¤ãAVSR(Audio Visual Speech Recognition)ãªã©ãŒã‚ã‚Šã¾ã™ã€‚

## ASR/VSRã®ç™ºå±•



ASRãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰
https://huggingface.co/spaces/hf-audio/open_asr_leaderboard

<!-- ã“ã‚Œå¤šè¨€èªã®ã»ã†ãŒä½ã„ã®ã¯ãªã‚“ã§ï¼Ÿ -->

https://superbbenchmark.github.io?subset=Paper#/leaderboard

VLM
https://huggingface.co/spaces/opencompass/open_vlm_leaderboard

### ãƒ¢ãƒ‡ãƒ«ä¸€è¦§

- ã‚¿ã‚¹ã‚¯ã¯ASVã‹VSRã®ã„ãšã‚Œã‹ã®ã¿è¨˜è¼‰ã—ã€AVT, VSR, AVSRãªã©ã¯çœç•¥ã—ãŸã€‚
- ç­†è€…ã®ä¸»è¦³ã§ä»£è¡¨çš„ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸ã‚“ã ã€‚
- ASRãƒ¢ãƒ‡ãƒ«ã«ã¯è¤‡æ•°ã®äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ãŒå­˜åœ¨ã™ã‚‹ãŸã‚ã€[Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)ã®è¨˜è¼‰ã‚’å‚ç…§ã—ãŸã€‚
- VSRãƒ¢ãƒ‡ãƒ«ã§ã¯ä¸»ã«LRS3ã®WERã‚’è¨˜è¼‰ã—ãŸã€‚ASRã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿ã®å¤šæ§˜æ€§ãŒä¹ã—ã„ãŸã‚æ•°å€¤ã®å˜ç´”æ¯”è¼ƒã‚’ã™ã¹ãã§ãªã„ç‚¹ã«æ³¨æ„ã€‚
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åå‰ã«ã¤ã„ã¦ã¯ã€æ¬¡ã®ã¨ãŠã‚Šçœç•¥ã—ãŸã€‚
  - LL: Libri-Light
  - LS: LibriSpeech
  - VC2: VoxCeleb2
  - YT: YouTube

|å¹´æœˆ|ã‚¿ã‚¹ã‚¯|ãƒ¢ãƒ‡ãƒ«|ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£|å­¦ç¿’ãƒ‡ãƒ¼ã‚¿|WER|ã‚³ãƒ¡ãƒ³ãƒˆ|
|-|-|-|-|-|-|-|
|2020|ASR|wav2vec 2.0|CNN+TF(Enc)|LL(60,000h)+LS(FT,960h)|22.55%||
|2020-12|VSR|VTP|3dCNN+VTP+TF(Enc/Dec)|LRS2+LRS3+TEDxext(2,676h)|30.7%||
|2021|ASR|HuBERT|CNN+TF(Enc)|LL(60,000h)+LS(FT,960h)|22.55%||
|2021|-|WavLM|
|2022|**VSR**|AV-HuBERT|CNN+TF(Enc)|VC2(1,326h)+LRS3(433h)+LRS3(FT,433h)|26.9%||
|2022|ASR|Whisper v1||680,000h|10.32%||
|2022-10|**VSR**|Ma et al.|3dCNN+2dCNN+CF+TF(Dec)|1,459h|31.5%||
|2023-04|**VSR**|RAVEn|||||
|2023-06|**VSR**|Auto-AVSR|3dCNN+RN18+CF+TF(Dec)||||
|2023|**VSR**|LP-Conformer|Frontend+Conformer|YT(100,000h)+LRS3(FT,400h)|12.8%|VSRã®SOTA|
|2023|ASR|Whisper large-v3||5,000,000h|7.44%||
|2024|ASR|Granite-Speech|||5.74%||
|2024|**VSR**|BRAVEn|||||
|2024-05|**VSR**|VSP-LLM|AV-HuBERT+Llama|VC2(1,326h)+LRS3(433h)+LRS3(FT,433h)|25.4%||
|2024-09|**VSR**|Whisperer|AV-HuBERT+Adpt+Whisper|VC2(1,326h)+LRS3(433h)+LRS3(FT,433h)|24.3%||
|2025|**VSR**|VALLR|ViT-Base+Adpt+CTC+Llama|(ViT-Base+Llama3.2-3B)+30h|18.7%||

<!-- ã¡ãªã¿ã«ãƒ—ãƒ­ã®Lip Readerã®æ­£è§£ç‡ã¯ï¼Ÿ -->

### æ•™å¸«ã‚ã‚Šå­¦ç¿’

è¶…ãƒ»å¤å…¸çš„ãªæŠ€è¡“


DNN


RNNãƒ™ãƒ¼ã‚¹ã®Makino 2019



### è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ãƒ»åæ•™å¸«ã‚ã‚Šå­¦ç¿’





#### wav2vec[^wav2vec]

[^wav2vec]: [S. Schneider, A. Baevski, R. Collobert, and M. Auli, â€œwav2vec: Unsupervised Pre-training for Speech Recognition,â€ Sept. 11, 2019, arXiv: arXiv:1904.05862. doi: 10.48550/arXiv.1904.05862.](https://arxiv.org/abs/1904.05862)


CNNãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•

#### VTP[^VTP]

[^VTP]: K. R. Prajwal, T. Afouras, and A. Zisserman, â€œSub-word Level Lip Reading With Visual Attention,â€ Dec. 03, 2021, arXiv: arXiv:2110.07603. doi: 10.48550/arXiv.2110.07603.

3D CNN + Visual Transformer Pooling + Transformer Encoder-Decoderã«ã‚ˆã‚‹ã€æ–‡å­—å˜ä½ã§ã¯ãªãã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å˜ä½ã®èªè­˜ãŒç‰¹å¾´ã®ãƒ¢ãƒ‡ãƒ«ã€‚VTPã¨ã„ã†ç•¥ç§°ã¯å…¬å¼ã§ã¯ãªã„ãŒã€å°‘ãªãã¨ã‚‚VALLR[^VALLR]ã§ãã†è¨€åŠã•ã‚Œã¦ã„ã‚‹ã€‚

#### wav2vec 2.0[^wav2vec2.0]

[^wav2vec2.0]: [A. Baevski, H. Zhou, A. Mohamed, and M. Auli, â€œwav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,â€ Oct. 22, 2020, arXiv: arXiv:2006.11477. doi: 10.48550/arXiv.2006.11477.](https://arxiv.org/abs/2006.11477)


https://huggingface.co/speechbrain/asr-wav2vec2-librispeech
https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self

transformer

#### HuBERT

https://huggingface.co/facebook/hubert-xlarge-ls960-ft


LRS3ã®WER 26.9%
åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã‚‚ä½¿ã‚ã‚Œã‚‹ãŒã€æ–‡å­—èµ·ã“ã—ã‚‚æ™®é€šã«ã§ãã‚‹
https://chatgpt.com/g/g-p-68e8381a0b7081918da1bbc98d7d060b-vsr/project

? BERTã®å‹ã¨ã—ã¦ã¯ï¼Ÿã¡ã‚‡ã£ã¨å¤ã„ã®ã‹ï¼Ÿ
RoPE+Pre-LN+GeGLUã€
äº¤äº’Attentionï¼ˆãƒ­ãƒ¼ã‚«ãƒ«çª“ï¼‹æ•£ç™ºã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼‰ã€
Unpaddingï¼‹FlashAttention ã€ã¨ã„ã£ãŸå·¥å¤«ã¯ãªã‹ã£ãŸæ™‚ä»£

å®Ÿè£…

ã“ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ã£ã¦ã„ã‚‹ä¸–ä»£ã¯...
VSP-LLM

RAVEn / BRAVEn
HuBERT + BYOL...ã‚‰ã—ã„

WavLM
é›¢æ•£éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã®ä¸€ã¤ã‚‰ã—ã„ãŒã€ä¸æ€è­°ã¨è¦‹ãªã„...?

#### AV-HuBERT[^AV-HuBERT]

[^AV-HuBERT]: [B. Shi, W.-N. Hsu, K. Lakhotia, and A. Mohamed, â€œLearning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction,â€ Mar. 13, 2022, arXiv: arXiv:2201.02184. doi: 10.48550/arXiv.2201.02184.](https://arxiv.org/abs/2201.02184)

HuBERTã‚’VSR/AVSRã«æ‹¡å¼µã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ 4å€‹åˆ†(=40ms)ã‹ã‚‰ãªã‚‹1éŸ³å£°ãƒ™ã‚¯ãƒˆãƒ«ã«å¯¾ã—ã¦ã€æ˜ åƒã‚’25fpsã§æ‰±ã£ã¦1æ˜ åƒãƒ•ãƒ¬ãƒ¼ãƒ (=40ms)ã‚’ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«çµåˆã—ã€Transformerã«å…¥åŠ›ã—ã¦ã„ã‚‹ã€‚

å…¬é–‹ã•ã‚Œã¦ã„ã‚‹äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯æ¬¡ã®é€šã‚Šã€‚

- [Metaå…¬å¼](https://facebookresearch.github.io/av_hubert/): VSR/AVSR, LRS3 + VoxCeleb2ãªã©
- [MuAViC](https://github.com/facebookresearch/muavic): AVSR, å¤šè¨€èª

æ„Ÿæƒ³ã«ãªã‚‹ãŒã€ç‰¹ã«LRS3 + VoxCeleb2ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«ã¯WERã»ã©ã®æ€§èƒ½ã¯ãªã„ã‹ã‚‚ã—ã‚Œãªã„ã€‚AV-HuBERTã®åŸ‹ã‚è¾¼ã¿ã‚’å…¥åŠ›ã«ç”¨ã„ãŸåˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã§è©¦ã—ãŸçµŒé¨“ã‹ã‚‰è¨€ã†ã¨ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã„è©±è€…ï¼ˆç§è‡ªèº«ã‚„ãƒ©ãƒ³ãƒ€ãƒ ãªè‹±èªã®æ˜ åƒï¼‰ã®è¦–è¦šéŸ³å£°èªè­˜ã¯ã¾ãšæˆåŠŸã—ãªã„ã€‚

#### Whisper[^Whisper]

[^Whisper]: A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, â€œRobust Speech Recognition via Large-Scale Weak Supervisionâ€.

2022å¹´ã«OpenAIãŒå…¬é–‹ã—ãŸASRãƒ¢ãƒ‡ãƒ«ã€‚

PyTorchå®Ÿè£…ã§æ‰±ã„ã‚„ã™ãã€ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã‚‚å……å®Ÿã—ã¦ã„ã‚‹ã€‚æœ€è¿‘ã ã¨[FFmpegã«Whisperã‚’ç”¨ã„ãŸæ–‡å­—èµ·ã“ã—æ©Ÿèƒ½ãŒæ­è¼‰ã•ã‚Œã‚‹ã¨ã—ã¦è©±é¡Œã«ãªã£ãŸã€‚](https://gigazine.net/news/20250814-ffmpeg-whisper-transcription)

#### Ma et al. (2022) VSR model[^Ma-VSR]

[^Ma-VSR]: P. Ma, S. Petridis, and M. Pantic, â€œVisual Speech Recognition for Multiple Languages in the Wild,â€ Nat Mach Intell, vol. 4, no. 11, pp. 930â€“939, Oct. 2022, doi: 10.1038/s42256-022-00550-z.

https://www.youtube.com/watch?v=FIau-6JA9Po

#### Auto-AVSR[^Auto-AVSR]

[^Auto-AVSR]: P. Ma, A. Haliassos, A. Fernandez-Lopez, H. Chen, S. Petridis, and M. Pantic, â€œAuto-AVSR: Audio-Visual Speech Recognition with Automatic Labels,â€ in ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), June 2023, pp. 1â€“5. doi: 10.1109/ICASSP49357.2023.10096889.

[äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿](https://github.com/mpc001/auto_avsr)ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿3,291æ™‚é–“ã§å­¦ç¿’ã—ãŸé‡ã¿ãŒã‚ã‚Šã€ã“ã‚Œã¯VSRã®å…¬é–‹ã•ã‚Œã¦ã„ã‚‹é‡ã¿ã®ä¸­ã§ã‚‚æœ€å¤§ã€‚

#### LP-Conformer[^LP-Conformer]

[^LP-Conformer]: [O. Chang, H. Liao, D. Serdyuk, A. Shah, and O. Siohan, â€œConformers are All You Need for Visual Speech Recognition,â€ Dec. 13, 2023, arXiv: arXiv:2302.10915. doi: 10.48550/arXiv.2302.10915.](https://arxiv.org/pdf/2302.10915)

LP-Conformerã¨ã„ã†åå‰ã¯è«–æ–‡å†…ã«ã¯ãªãã€ãƒ–ãƒ­ã‚°ãªã©ã§å‘¼ã°ã‚Œã¦ã„ã‚‹ã‚‚ã®ã€‚[^anwarvic-LP_Conformer]
[^anwarvic-LP_Conformer]: https://anwarvic.github.io/speech-recognition/LP_Conformer

#### NVIDIA Canary (2024) - Comformer+LLM

è¨€ã£ã¦ã¿ã‚Œã°wav2vec2+llmã£ã¦ã“ã¨ï¼Ÿ â†’ transformer-decoderã‚‰ã—ã„
NeMOã‚·ãƒªãƒ¼ã‚ºã®Cannary
Comformeréƒ¨åˆ†ã ã‘ä½¿ã£ãŸã‚‰ã„ã„æ„Ÿã˜ã®ãƒ­ãƒ¼ãƒå­—ãŒå–å¾—ã§ããŸã‚Šã™ã‚‹ã®ã‹...?
Comformer+Llamaã¯ãªã„ã®ã‹ï¼Ÿ

å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯86kæ™‚é–“

#### Microsoft Phi-4 Multimodal Instruct (2024â€“25) - ç›´æ¥éŸ³å£°ã‚’ç†è§£
ã™ã§ã«ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«...

#### IBM Granite Speech

#### Whisperer[^Whisperer]

[^Whisperer]: K. R. Prajwal, T. Afouras, and A. Zisserman, â€œSpeech Recognition Models are Strong Lip-readers,â€ in Interspeech 2024, ISCA, Sept. 2024, pp. 2425â€“2429. doi: 10.21437/Interspeech.2024-2290.



#### VALLR[^VALLR]

[^VALLR]: M. Thomas, E. Fish, and R. Bowden, â€œVALLR: Visual ASR Language Model for Lip Reading,â€ Mar. 27, 2025, arXiv: arXiv:2503.21408. doi: 10.48550/arXiv.2503.21408.

ViT-Baseã‚’çµ„ã¿è¾¼ã‚“ã éŸ³ç´ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’é–‹ç™ºã—ã€Llama3ç­‰ã¨æ¥ç¶šã€‚å…·ä½“çš„ã«ã¯ã€æ˜ åƒã®å„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é™æ­¢ç”»ã¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã€æ™‚ç³»åˆ—æ–¹å‘ã«1æ¬¡å…ƒç•³ã¿è¾¼ã¿ã€ãã®å¾ŒCTCå‡¦ç†ã‚’åŠ ãˆã¦ã„ã‚‹ã€‚

ViT-Baseã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã ã‘ã§ãªãã€äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ä½¿ã£ã¦ã“ãã®æ€§èƒ½ã¨æ€ã‚ã‚Œã‚‹ã€‚LRS3ã®ã‚ãšã‹30æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã‚‹ã¨ã„ã†å‰æã®ãŸã‚ViT-Baseã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã®ã‹ã‚‚ã—ã‚Œãªã„ã€‚ãã®å ´åˆã€å­¦ç¿’ãƒ»æ¨è«–é€Ÿåº¦ã‚’ç„¡è¦–ã™ã‚Œã°ã€ã‚ˆã‚Šå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ + ViT-Largeã®æ¡ç”¨ã§ç°¡å˜ã«æ€§èƒ½ã®å‘ä¸ŠãŒæœŸå¾…ã§ãã‚‹ã€‚

<!--
çµæ§‹ç†ã«é©ã£ã¦ã„ã‚‹æ°—ãŒã™ã‚‹ã®ã ã‘ã©ã€ViTæ¡ç”¨ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã©ã“ã‹ã‚‰ï¼Ÿ
ï¼ˆä»–ã®ãƒ¢ãƒ‡ãƒ«ãŒAV-HuBERTã«ã“ã ã‚ã£ã¦ã„ã‚‹ã®ã¯ãªãœï¼Ÿï¼‰
ï¼ˆ9æœˆã«VideoMAEã§è‡ªä½œã—ãŸéš›ã¯ã‚ã¾ã‚Šã†ã¾ãã„ã‹ãªã‹ã£ãŸãŒã€ã“ã®è«–æ–‡ã¨ã®å·®åˆ†ã¯ï¼Ÿï¼‰

ä»–ã®è«–æ–‡ã¸ã®è¨€åŠ
- VSP-LLM, Personal Lip Reading: LLMæ´»ç”¨ã®å…ˆé§†è€…ã¨ã—ã¦è¨€åŠã€‚ãŸã ã—åˆ¶å¾¡æ€§ã®ä½ã•ã‚’æŒ‡æ‘˜ã€‚Zero-AVSRã¯ã»ã¼ç™»å ´ãŒåŒæ™‚ãªã®ã§å¼•ç”¨ã•ã‚Œã¦ã„ãªã„ã€‚
- VTP, LP: ã‚¹ã‚³ã‚¢ã®ã¿
- Whisperer: ASRãƒ¢ãƒ‡ãƒ«ã®è»¢ç§»å­¦ç¿’ã®æœ€æ–°æ‰‹æ³•ã¨ã—ã¦ç´¹ä»‹ã—ã€å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ãªç‚¹ã‚’æŒ‡æ‘˜ã€‚ã»ã‹ã€CTCãƒ˜ãƒƒãƒ‰ã®å­¦ç¿’æˆ¦ç•¥ã®ãã£ã‹ã‘ã¨ã—ã¦è¨€åŠ
- Zero-shot keyword spotting: å…ˆé§†ã‘ã¨ã—ã¦ï¼ˆã®ã¯ãšï¼‰
-->

## VSRã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

|å¹´æœˆ|åå‰|è¨€èª|æ™‚é–“|ã‚½ãƒ¼ã‚¹|ç‰¹å¾´|
|-|-|-|-|-|-|
||GRID|è‹±èª|||
||LRW|è‹±èª|173h|||
|2016-11|LRS|è‹±èª|?h|BBC||
||AVSpeech|è‹±èª||||
|2018|VoxCeleb2|è‹±èª||||
|2018-09|LRS2|è‹±èª|224h|BBC||
|2018-09|LRS3|è‹±èª|437h|TED&TEDx||
||WildVSR||||
|2023-01|OLKAVS|éŸ“å›½èª|1,150h||


ãªãŠã€å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å›³è¡¨ã¯å…¬å¼ãŠã‚ˆã³ã‚µãƒ¼ãƒ™ã‚¤[^Changchong]ã‹ã‚‰ã®å¼•ç”¨ã§ã‚ã‚‹ã€‚

[^Changchong]: [1] S. Changchong et al., â€œDeep Learning for Visual Speech Analysis: A Survey.â€ Accessed: Oct. 28, 2025. [Online]. Available: https://www.computer.org/csdl/journal/tp/2024/09/10472054/1VhFlotHb2w

### LRW[^LRW]

[^LRW]: [J. S. Chung and A. Zisserman, â€œLip Reading in the Wild,â€ S.-H. Lai, V. Lepetit, K. Nishino, and Y. Sato, Eds., in Lecture Notes in Computer Science, vol. 10112. Cham: Springer International Publishing, 2017, pp. 87â€“103. doi: 10.1007/978-3-319-54184-6_6.](https://link.springer.com/chapter/10.1007/978-3-319-54184-6_6)

### LRS[^LRS]

[^LRS]: [J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, â€œLip Reading Sentences in the Wild,â€ in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017, pp. 3444â€“3453. doi: 10.1109/CVPR.2017.367.](https://arxiv.org/abs/1611.05358)

![LRS](/images/vsr-visual-speech-recognition-survey/lrs-fig3.png)

LRWãŒ1ã‚¯ãƒªãƒƒãƒ—1å˜èªã ã£ãŸã®ã«å¯¾ã—ã¦ã€1ã‚¯ãƒªãƒƒãƒ—1ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã¨ãªã£ã¦ã„ã‚‹ã€‚

### LRS2[^LRS2]

[^LRS2]: [T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, â€œDeep Audio-Visual Speech Recognition,â€ IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 12, pp. 8717â€“8727, Dec. 2022, doi: 10.1109/TPAMI.2018.2889052.](https://arxiv.org/abs/1809.02108)

![LRS2](/images/vsr-visual-speech-recognition-survey/lrs2-fig3.png)

LRS1ã®ä¸Šä½äº’æ›ã¨è¦‹åšã•ã‚Œã¦ã„ã‚‹ã®ã‹ã€LRS1ã‚’é£›ã°ã—ã¦LRS2ãŒè¨€åŠã•ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã€‚ç¾åœ¨ã¯å…¬å¼ã‚µã‚¤ãƒˆã®é…å¸ƒãŒåœæ­¢ã—ã¦ã„ã‚‹ã€‚

### LRS3[^LRS3]

[^LRS3]: [T. Afouras, J. S. Chung, and A. Zisserman, â€œLRS3-TED: a large-scale dataset for visual speech recognition,â€ Oct. 28, 2018, arXiv: arXiv:1809.00496. doi: 10.48550/arXiv.1809.00496.](https://arxiv.org/abs/1809.00496)

![LRS3](/images/vsr-visual-speech-recognition-survey/dl-for-vsa-lrs3.png)

2025å¹´æ™‚ç‚¹ã«ãŠã‘ã‚‹VSRã®äº‹å®Ÿä¸Šã®æ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚ç¾åœ¨ã¯[å…¬å¼ã‚µã‚¤ãƒˆ](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/)ã®é…å¸ƒãŒåœæ­¢ã—ã¦ã„ã‚‹ã€‚Train/Val/Testã§è©±è€…åˆ†é›¢ã‚’ã—ã¦ã„ãªã„ãŸã‚ã€LRS3ã®ã¿ã§å­¦ç¿’ã™ã‚‹ã¨è©±è€…ã”ã¨ã®å……åˆ†ãªæ±åŒ–æ€§èƒ½ãŒå¾—ã‚‰ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

TEDã®æ˜ åƒã‹ã‚‰è‡ªå‹•ã§é¡”èªè­˜ã—ã¦ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆã—ã¦ã„ã‚‹ã€‚ç™»å£‡è€…ã§ã¯ãªãèƒŒå¾Œã®ã‚¹ãƒ©ã‚¤ãƒ‰ãŒåˆ‡ã‚ŠæŠœã‹ã‚Œã¦ã—ã¾ã£ã¦ã„ã‚‹ã‚¯ãƒªãƒƒãƒ—ã‚‚æ„å¤–ã¨ã‚ã‚‹ã‚‰ã—ã„ï¼Ÿ

### WildVSR[^WildVSR]

[^WildVSR]: Y. A. D. Djilali, S. Narayan, E. L. Bihan, H. Boussaid, E. Almazrouei, and M. Debbah, â€œDo VSR Models Generalize Beyond LRS3?,â€ Nov. 23, 2023, arXiv: arXiv:2311.14063. doi: 10.48550/arXiv.2311.14063.

<!-- è§’åº¦ã¨ã‹ã¯ã“ã ã‚ã‚Šã‚ã‚‹ï¼Ÿ -->

### OLKAVS[^OLKAVS]

[^OLKAVS]: [J. Park et al., â€œOLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,â€ Aug. 28, 2025, arXiv: arXiv:2301.06375. doi: 10.48550/arXiv.2301.06375.]([1] J. Park et al., â€œOLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,â€ Aug. 28, 2025, arXiv: arXiv:2301.06375. doi: 10.48550/arXiv.2301.06375.)

éè‹±èªã€ç·æ˜ åƒæ™‚é–“1,150æ™‚é–“ã¨ã„ã†ã ã‘ã§ã‚‚ã™ã”ã„ã®ã«ã€5ç‚¹ã‹ã‚‰åéŒ²ã—ã¦ã„ã‚‹ã®ã§å®Ÿè³ªãƒ‡ãƒ¼ã‚¿é‡ãŒ5å€ã€‚

## VSRã®å­¦ç¿’

æ™‚é–“ãƒã‚¹ã‚­ãƒ³ã‚°
(ma et all 2022ãªã©)
æ–‡è„ˆæƒ…å ±ã«å¿œã˜ã¦å£ã®ã‹ãŸã¡ã‚’æ¨å®šã™ã‚‹åŠ›ã‚’é¤Šã†

ãƒ¢ãƒ€ãƒªãƒ†ã‚£dropout

æ“¬ä¼¼ãƒ©ãƒ™ãƒ«

è»¢ç§»å­¦ç¿’ãƒ¡ã‚½ãƒƒãƒ‰

å¤šè¨€èªå¯¾å¿œãƒ¡ã‚½ãƒƒãƒ‰

### QLoRA, LoRA, ãƒ•ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

VSP-LLMã‚’ç”¨ã„ã¦ã€LLMã®è¿½åŠ å­¦ç¿’æ–¹æ³•ã«ã¤ã„ã¦æ¯”è¼ƒãŒãªã•ã‚Œã¦ã„ã¾ã™ã€‚LRS3ã«ã‚ˆã‚‹å­¦ç¿’ã«ãŠã„ã¦ã¯ã€å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒ30kã‚¹ãƒ†ãƒƒãƒ—æ•°ç¨‹åº¦ã§ã‚ã‚‹å ´åˆã€QLoRAã‚„LoRAã§ã¯æå¤±ãŒåæŸã™ã‚‹ãŒã€ãƒ•ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯åæŸã«è‡³ã‚‰ãªã„ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

## ä»Šå¾Œã®èª²é¡Œ

- ãƒ¢ãƒ‡ãƒ«ã¯æ€§èƒ½ã®è‰¯ã•ã¨ã„ã†ã‚ˆã‚Šå­¦ç¿’åŠ¹ç‡ã§é¸ã‚“ã æ–¹ãŒè‰¯ã„ã‹ã‚‚ã—ã‚Œãªã„
(AV-HuBERT? ViT? VideoMAE?)
(Phonemeãƒ™ãƒ¼ã‚¹? ãƒˆãƒ¼ã‚¯ãƒ³ãƒ™ãƒ¼ã‚¹?)
(ç›´æ¥çµ±åˆ? ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµŒç”±ã®çµ±åˆ?)
(ã‚»ãƒŸã‚ªãƒ¼ãƒ—ãƒ³ãªèªå½™ã®å ´åˆã¯ï¼Ÿ)

## å‚è€ƒæ–‡çŒ®ï¼ˆã‚µãƒ¼ãƒ™ã‚¤ãªã©ï¼‰

- ASR
  - [C. Wu, Y. Pan, H. Wu, and L. Ning, â€œIntegrating Speech Recognition into Intelligent Information Systems: From Statistical Models to Deep Learning,â€ Informatics, vol. 12, no. 4, p. 107, Oct. 2025, doi: 10.3390/informatics12040107.](https://www.mdpi.com/2227-9709/12/4/107)
  - [Y. Yang et al., â€œTowards Universal Speech Discrete Tokens: A Case Study for ASR and TTS,â€ Dec. 14, 2023, arXiv: arXiv:2309.07377. doi: 10.48550/arXiv.2309.07377.](https://arxiv.org/abs/2309.07377)
- VSR
  - [J. Rishabh and H. Naomi, â€œFrom Hype to Insight: Rethinking Large Language Model Integration in Visual Speech Recognition.â€ Accessed: Oct. 27, 2025. [Online]. Available: https://arxiv.org/abs/2509.14880v1](https://arxiv.org/abs/2509.14880v1): ã‚µãƒ¼ãƒ™ã‚¤ã§ã¯ãªãè«–æ–‡ã§ã™ãŒã€æœ€è¿‘ã®VSR+LLMã®æµã‚Œã‚’ã¾ã¨ã‚ã¦ã„ã‚‹ï¼ˆã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹ã§ã¯ï¼‰ç¾çŠ¶å”¯ä¸€ã®è«–æ–‡ã‹ã‚‚ï¼Ÿ
  - [P. Nemani, G. S. Krishna, and S. Kundrapu, â€œAutomated Speaker Independent Visual Speech Recognition: A Comprehensive Survey,â€ Image and Vision Computing, vol. 138, p. 104787, Oct. 2023, doi: 10.1016/j.imavis.2023.104787.](https://arxiv.org/abs/2306.08314): åŒ…æ‹¬çš„ã€‚ã•ã‚‰ã«OLKAVSã«è¨€åŠã—ã¦ã„ã‚‹æ•°å°‘ãªã„è«–æ–‡ã€‚
  - [K. Rezaee and M. Yeganeh, â€œAutomatic Visual Lip Reading: A Comparative Review of Machine-Learning Approaches,â€ Results in Engineering, p. 107171, Sept. 2025, doi: 10.1016/j.rineng.2025.107171.](https://www.sciencedirect.com/science/article/pii/S2590123025032268)
- Computer Vision
  - [N. Madan, A. Moegelmose, R. Modi, Y. S. Rawat, and T. B. Moeslund, â€œFoundation Models for Video Understanding: A Survey,â€ May 06, 2024, arXiv: arXiv:2405.03770. doi: 10.48550/arXiv.2405.03770.](http://arxiv.org/abs/2405.03770)
