---
title: 'èª­å”‡è¡“ã®ç ”ç©¶ã¾ã¨ã‚ã€ã‚µãƒ¼ãƒ™ã‚¤ã€‘'
emoji: "ğŸ’„"
type: "tech"
topics: ["vsr", "machinelearning", "deeplearning", "LLM"]
published: false
references:
- OpenASR Leaderboard: https://chatgpt.com/c/68faf914-d57c-8322-ac80-2f178b8eae6e?ref=mini
- https://chatgpt.com/c/68fb1d9c-1540-8322-9307-4bef509a67f8?ref=mini
- RAVEn/BRAVEn: https://chatgpt.com/c/68fb298e-7ccc-8320-a573-c31bf48e2bad?ref=mini
- å¼±æ•™å¸«ã‚ã‚Šå­¦ç¿’ãªã©: https://chatgpt.com/c/68fd58f1-24b8-8323-bdd8-5832a50ae0ce?ref=mini
- LP-Conformer: https://chatgpt.com/c/68fd5331-2e30-8320-bd0d-817e1a448489?ref=mini
---

## TL;DR

## å‹•æ©Ÿ

èª­å”‡è¡“ã®ãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºã™ã‚‹ãŸã‚ã«ã¯ã€è¦–è¦šéŸ³å£°èªè­˜(VSR)ã ã‘ã§ãªãéŸ³å£°èªè­˜(ASR)ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æŠ‘ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãã“ã§ã€ASR/VSRã«ãŠã‘ã‚‹ä»£è¡¨çš„ãªãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»å­¦ç¿’æ‰‹æ³•ãªã©ã«ã¤ã„ã¦ã¾ã¨ã‚ã¾ã—ãŸã€‚

æ™‚é–“ã‚’ã‹ã‘ã¦æ³¨æ„æ·±ãèª¿ã¹ã¦ã„ã¾ã™ãŒã€ç­†è€…ã¯å°‚é–€çš„ãªè¨“ç·´ã‚’å—ã‘ã¦ã„ãªã„ãŸã‚ã€æœ¬è¨˜äº‹ã«ã¯èª¤ã‚ŠãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

## ASR/VSRã®ç™ºå±•ã®æ¦‚è¦

ASRãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰
https://huggingface.co/spaces/hf-audio/open_asr_leaderboard

<!-- ã“ã‚Œå¤šè¨€èªã®ã»ã†ãŒä½ã„ã®ã¯ãªã‚“ã§ï¼Ÿ -->

VLM
https://huggingface.co/spaces/opencompass/open_vlm_leaderboard


## æ­´å²

è¶…ãƒ»å¤å…¸çš„ãªæŠ€è¡“


DNN


RNNãƒ™ãƒ¼ã‚¹ã®Makino 2019



### è‡ªå·±æ•™å¸«ã‚ã‚Šæ™‚ä»£

- ã‚¿ã‚¹ã‚¯ã¯ASVã‹VSRã®ã„ãšã‚Œã‹ã®ã¿è¨˜è¼‰ã—ã€AVT, VSR, AVSRãªã©ã¯çœç•¥ã—ãŸã€‚
- ç­†è€…ã®ä¸»è¦³ã§ä»£è¡¨çš„ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸ã‚“ã ã€‚
- ASRãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŠã‚ˆã³WERã«ã¤ã„ã¦ã¯ã€[Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆã—ãŸã€‚
- VSRãƒ¢ãƒ‡ãƒ«ã®WERã«ã¤ã„ã¦ã¯ã€è«–æ–‡ã®è¨˜è¼‰ã‚’å„ªå…ˆã—ãŸã€‚è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¤šæ§˜æ€§ã«ä¹ã—ã„ç‚¹ã«æ³¨æ„ã€‚
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åå‰ã«ã¤ã„ã¦ã¯ã€æ¬¡ã®ã¨ãŠã‚Šçœç•¥ã—ãŸã€‚
  - LL: Libri-Light
  - LS: LibriSpeech
  - VC2: VoxCeleb2
  - YT: YouTube

|å¹´æœˆ|ã‚¿ã‚¹ã‚¯|ãƒ¢ãƒ‡ãƒ«|ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£|å­¦ç¿’ãƒ‡ãƒ¼ã‚¿|WER|ã‚³ãƒ¡ãƒ³ãƒˆ|
|-|-|-|-|-|-|-|
|2020|ASR|wav2vec 2.0|CNN+Transformer(Enc)|LL(60,000h)+LS(FT,960h)|22.55%||
|2021|ASR|HuBERT|CNN+Transformer(Enc)|LL(60,000h)+LS(FT,960h)|22.55%||
|2022|**VSR**|AV-HuBERT|CNN+Transformer(Enc)|VC2(1,326h)+LRS3(433h)+LRS3(FT,433h)|26.9%||
|2022|ASR|Whisper v1||680,000h|10.32%||
|2022-10|**VSR**|Ma et al. (2022)|3dCNN+2dCNN+CF+TF(Dec)|1,459h|31.5%||
|2023|**VSR**|LP-Conformer|Frontend+Conformer|YT(100,000h)+LRS3(FT,400h)|12.8%|VSRã®SOTA|
|2023|ASR|Whisper large-v3||5,000,000h|7.44%||
|2024|ASR|Granite-Speech|||5.74%||
|2024-05|**VSR**|VSP-LLM|AV-HuBERT+Llama|VC2(1,326h)+LRS3(433h)+LRS3(FT,433h)|25.4%||
|2024-09|**VSR**|Whisperer|AV-HuBERT+Adpt+Whisper|VC2(1,326h)+LRS3(433h)+LRS3(FT,433h)|24.3%||
|2025|**VSR**|VALLR|ViT-Base+Adpt+CTC+Llama|(ViT-Base+Llama3.2-3B)+30h|18.7%||


#### wav2vec

CNNãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•

#### wav2vec 2.0

https://huggingface.co/speechbrain/asr-wav2vec2-librispeech
https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self

transformer

#### HuBERT

https://huggingface.co/facebook/hubert-xlarge-ls960-ft


LRS3ã®WER 26.9%
åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã‚‚ä½¿ã‚ã‚Œã‚‹ãŒã€æ–‡å­—èµ·ã“ã—ã‚‚æ™®é€šã«ã§ãã‚‹
https://chatgpt.com/g/g-p-68e8381a0b7081918da1bbc98d7d060b-vsr/project

? BERTã®å‹ã¨ã—ã¦ã¯ï¼Ÿã¡ã‚‡ã£ã¨å¤ã„ã®ã‹ï¼Ÿ
RoPE+Pre-LN+GeGLUã€
äº¤äº’Attentionï¼ˆãƒ­ãƒ¼ã‚«ãƒ«çª“ï¼‹æ•£ç™ºã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼‰ã€
Unpaddingï¼‹FlashAttention ã€ã¨ã„ã£ãŸå·¥å¤«ã¯ãªã‹ã£ãŸæ™‚ä»£

å®Ÿè£…

ã“ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ã£ã¦ã„ã‚‹ä¸–ä»£ã¯...
VSP-LLM

RAVEn / BRAVEn
HuBERT + BYOL...ã‚‰ã—ã„

WavLM
é›¢æ•£éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã®ä¸€ã¤ã‚‰ã—ã„ãŒã€ä¸æ€è­°ã¨è¦‹ãªã„...?

#### AV-HuBERT[^AV-HuBERT]

[^AV-HuBERT]: [B. Shi, W.-N. Hsu, K. Lakhotia, and A. Mohamed, â€œLearning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction,â€ Mar. 13, 2022, arXiv: arXiv:2201.02184. doi: 10.48550/arXiv.2201.02184.](https://arxiv.org/abs/2201.02184)

HuBERTã‚’VSR/AVSRã«æ‹¡å¼µã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ 4å€‹åˆ†(=40ms)ã‹ã‚‰ãªã‚‹1éŸ³å£°ãƒ™ã‚¯ãƒˆãƒ«ã«å¯¾ã—ã¦ã€æ˜ åƒã‚’25fpsã§æ‰±ã£ã¦1æ˜ åƒãƒ•ãƒ¬ãƒ¼ãƒ (=40ms)ã‚’ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«çµåˆã—ã€Transformerã«å…¥åŠ›ã—ã¦ã„ã‚‹ã€‚

å…¬é–‹ã•ã‚Œã¦ã„ã‚‹äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯æ¬¡ã®é€šã‚Šã€‚

- [Metaå…¬å¼](https://facebookresearch.github.io/av_hubert/): VSR/AVSR, LRS3 + VoxCeleb2ãªã©
- [MuAViC](https://github.com/facebookresearch/muavic): AVSR, å¤šè¨€èª

æ„Ÿæƒ³ã«ãªã‚‹ãŒã€ç‰¹ã«LRS3 + VoxCeleb2ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«ã¯WERã»ã©ã®æ€§èƒ½ã¯ãªã„ã‹ã‚‚ã—ã‚Œãªã„ã€‚AV-HuBERTã®åŸ‹ã‚è¾¼ã¿ã‚’å…¥åŠ›ã«ç”¨ã„ãŸåˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã§è©¦ã—ãŸçµŒé¨“ã‹ã‚‰è¨€ã†ã¨ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãªã„è©±è€…ï¼ˆç§è‡ªèº«ã‚„ãƒ©ãƒ³ãƒ€ãƒ ãªè‹±èªã®æ˜ åƒï¼‰ã®è¦–è¦šéŸ³å£°èªè­˜ã¯ã¾ãšæˆåŠŸã—ãªã„ã€‚

#### Whisper[^Whisper]

[^Whisper]: A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, â€œRobust Speech Recognition via Large-Scale Weak Supervisionâ€.

2022å¹´ã«OpenAIãŒå…¬é–‹ã—ãŸASRãƒ¢ãƒ‡ãƒ«ã€‚

PyTorchå®Ÿè£…ã§æ‰±ã„ã‚„ã™ãã€ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã‚‚å……å®Ÿã—ã¦ã„ã‚‹ã€‚æœ€è¿‘ã ã¨[FFmpegã«Whisperã‚’ç”¨ã„ãŸæ–‡å­—èµ·ã“ã—æ©Ÿèƒ½ãŒæ­è¼‰ã•ã‚Œã‚‹ã¨ã—ã¦è©±é¡Œã«ãªã£ãŸã€‚](https://gigazine.net/news/20250814-ffmpeg-whisper-transcription)

#### Ma et al. (2022) VSR model[^Ma-VSR]

[^Ma-VSR]: P. Ma, S. Petridis, and M. Pantic, â€œVisual Speech Recognition for Multiple Languages in the Wild,â€ Nat Mach Intell, vol. 4, no. 11, pp. 930â€“939, Oct. 2022, doi: 10.1038/s42256-022-00550-z.


#### LP-Conformer[^LP-Conformer]

[^LP-Conformer]: [O. Chang, H. Liao, D. Serdyuk, A. Shah, and O. Siohan, â€œConformers are All You Need for Visual Speech Recognition,â€ Dec. 13, 2023, arXiv: arXiv:2302.10915. doi: 10.48550/arXiv.2302.10915.](https://arxiv.org/pdf/2302.10915)

LP-Conformerã¨ã„ã†åå‰ã¯è«–æ–‡å†…ã«ã¯ãªãã€ãƒ–ãƒ­ã‚°ãªã©ã§å‘¼ã°ã‚Œã¦ã„ã‚‹ã‚‚ã®ã€‚[^anwarvic-LP_Conformer]
[^anwarvic-LP_Conformer]: https://anwarvic.github.io/speech-recognition/LP_Conformer

#### NVIDIA Canary (2024) - Comformer+LLM

è¨€ã£ã¦ã¿ã‚Œã°wav2vec2+llmã£ã¦ã“ã¨ï¼Ÿ â†’ transformer-decoderã‚‰ã—ã„
NeMOã‚·ãƒªãƒ¼ã‚ºã®Cannary
Comformeréƒ¨åˆ†ã ã‘ä½¿ã£ãŸã‚‰ã„ã„æ„Ÿã˜ã®ãƒ­ãƒ¼ãƒå­—ãŒå–å¾—ã§ããŸã‚Šã™ã‚‹ã®ã‹...?
Comformer+Llamaã¯ãªã„ã®ã‹ï¼Ÿ

å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯86kæ™‚é–“

#### Microsoft Phi-4 Multimodal Instruct (2024â€“25) - ç›´æ¥éŸ³å£°ã‚’ç†è§£
ã™ã§ã«ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«...

#### IBM Granite Speech

#### Whisperer[^Whisperer]

[^Whisperer]: K. R. Prajwal, T. Afouras, and A. Zisserman, â€œSpeech Recognition Models are Strong Lip-readers,â€ in Interspeech 2024, ISCA, Sept. 2024, pp. 2425â€“2429. doi: 10.21437/Interspeech.2024-2290.



#### VALLR[^VALLR]

[^VALLR]: M. Thomas, E. Fish, and R. Bowden, â€œVALLR: Visual ASR Language Model for Lip Reading,â€ Mar. 27, 2025, arXiv: arXiv:2503.21408. doi: 10.48550/arXiv.2503.21408.

ViT-Baseã‚’çµ„ã¿è¾¼ã‚“ã éŸ³ç´ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’é–‹ç™ºã—ã€Llama3ç­‰ã¨æ¥ç¶šã€‚å…·ä½“çš„ã«ã¯ã€æ˜ åƒã®å„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é™æ­¢ç”»ã¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã€æ™‚ç³»åˆ—æ–¹å‘ã«1æ¬¡å…ƒç•³ã¿è¾¼ã¿ã€ãã®å¾ŒCTCå‡¦ç†ã‚’åŠ ãˆã¦ã„ã‚‹ã€‚

ViT-Baseã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã ã‘ã§ãªãã€äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ä½¿ã£ã¦ã“ãã®æ€§èƒ½ã¨æ€ã‚ã‚Œã‚‹ã€‚LRS3ã®ã‚ãšã‹30æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã‚‹ã¨ã„ã†å‰æã®ãŸã‚ViT-Baseã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã®ã‹ã‚‚ã—ã‚Œãªã„ã€‚ãã®å ´åˆã€å­¦ç¿’ãƒ»æ¨è«–é€Ÿåº¦ã‚’ç„¡è¦–ã™ã‚Œã°ã€ã‚ˆã‚Šå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ + ViT-Largeã®æ¡ç”¨ã§ç°¡å˜ã«æ€§èƒ½ã®å‘ä¸ŠãŒæœŸå¾…ã§ãã‚‹ã€‚

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

|å¹´æœˆ|åå‰|æ™‚é–“|ã‚½ãƒ¼ã‚¹|ç‰¹å¾´|
|-|-|-|-|-|
||LRW||||
||LRS2||||
||AVSpeech||||
|2018|VoxCeleb2||||
||LRS3||||
||WildVSR|||



### LRS3

VSRã®äº‹å®Ÿä¸Šã®æ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚ç¾åœ¨ã¯å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰é…å¸ƒã•ã‚Œã¦ã„ãªã„ã€‚Train/Val/Testã§è©±è€…åˆ†é›¢ã‚’ã—ã¦ã„ãªã„ãŸã‚ã€LRS3ã®ã¿ã§å­¦ç¿’ã™ã‚‹ã¨è©±è€…ã”ã¨ã®å……åˆ†ãªæ±åŒ–æ€§èƒ½ãŒå¾—ã‚‰ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

<!-- LRS2ã‚ˆã‚Šã‚‚é›£ã—ã‹ã£ãŸã¯ãš -->

### WildVSR[^WildVSR]

[^WildVSR]: Y. A. D. Djilali, S. Narayan, E. L. Bihan, H. Boussaid, E. Almazrouei, and M. Debbah, â€œDo VSR Models Generalize Beyond LRS3?,â€ Nov. 23, 2023, arXiv: arXiv:2311.14063. doi: 10.48550/arXiv.2311.14063.

<!-- è§’åº¦ã¨ã‹ã¯ã“ã ã‚ã‚Šã‚ã‚‹ï¼Ÿ -->

## ASR to VSRã®æŠ€è¡“

æ™‚é–“ãƒã‚¹ã‚­ãƒ³ã‚°
(ma et all 2022ãªã©)
æ–‡è„ˆæƒ…å ±ã«å¿œã˜ã¦å£ã®ã‹ãŸã¡ã‚’æ¨å®šã™ã‚‹åŠ›ã‚’é¤Šã†

ãƒ¢ãƒ€ãƒªãƒ†ã‚£dropout

æ“¬ä¼¼ãƒ©ãƒ™ãƒ«

è»¢ç§»å­¦ç¿’ãƒ¡ã‚½ãƒƒãƒ‰

å¤šè¨€èªå¯¾å¿œãƒ¡ã‚½ãƒƒãƒ‰

## ä»Šå¾Œã®èª²é¡Œ

- ãƒ¢ãƒ‡ãƒ«ã¯æ€§èƒ½ã®è‰¯ã•ã¨ã„ã†ã‚ˆã‚Šå­¦ç¿’åŠ¹ç‡ã§é¸ã‚“ã æ–¹ãŒè‰¯ã„ã‹ã‚‚ã—ã‚Œãªã„

## ã‚µãƒ¼ãƒ™ã‚¤

VSR
K. Rezaee and M. Yeganeh, â€œAutomatic Visual Lip Reading: A Comparative Review of Machine-Learning Approaches,â€ Results in Engineering, p. 107171, Sept. 2025, doi: 10.1016/j.rineng.2025.107171.
ï¼ˆã§ã‚‚AV-HuBERTã«è§¦ã‚Œã¦ãªã„ã®ãŒæ°—ã«ãªã‚‹ï¼‰

Integrating Speech Recognition into Intelligent Information
Systems: From Statistical Models to Deep Learning


å‹•ç”»èªè­˜å´
[1] N. Madan, A. Moegelmose, R. Modi, Y. S. Rawat, and T. B. Moeslund, â€œFoundation Models for Video Understanding: A Survey,â€ May 06, 2024, arXiv: arXiv:2405.03770. doi: 10.48550/arXiv.2405.03770.
