---
title: "SDãƒ¢ãƒ‡ãƒ«ã‹ã‚‰CLIPã‚’å–ã‚Šå‡ºã—ã¦ä½¿ã†"
emoji: "ğŸ“"
type: "tech"
topics: ["CLIP", "StableDiffusion"]
published: true
chats:
- https://claude.ai/chat/19b185aa-b5be-49f1-a73b-9a88a38fab48
- https://claude.ai/chat/8134061b-e048-424f-96f1-a224ce43e764
---

## TL;DR

- Stable Diffusionã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰CLIP Textãƒ¢ãƒ‡ãƒ«ã‚’å–ã‚Šå‡ºãã†ã¨ã—ã¾ã—ãŸã€‚
- Stable Diffusionã¯CLIP Textãƒ¢ãƒ‡ãƒ«ã®ã™ã¹ã¦ã‚’ä½¿ã£ã¦ã„ã‚‹ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
- Stable Diffusionç”±æ¥ã®é‡ã¿ã§ViT/L-14ã®é‡ã¿ã‚’å·®ã—æ›¿ãˆã¦ã‚‚å‹•ãã“ã¨ã‚’ç¢ºèªã§ãã¾ã—ãŸã€‚

## å‹•æ©Ÿ

Stable Diffusionãƒ¢ãƒ‡ãƒ«ã®ç‰¹æ€§ã‚’èª¿ã¹ã‚‹ã†ãˆã§ã€CLIPScoreã¨ã„ã†æŒ‡æ¨™ãŒã‚ã‚‹ã“ã¨ã‚’çŸ¥ã‚Šã¾ã—ãŸã€‚

ãã“ã§ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«ã®CLIPScoreã‚’èª¿ã¹ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ„ã‚‚ã†ã¨æ€ã£ãŸã®ã§ã™ãŒã€ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦CLIPãŒç•°ãªã‚‹å¯èƒ½æ€§ã«æ€ã„å½“ãŸã‚Šã¾ã—ãŸã€‚ãã®ãŸã‚ã€æ­£ã—ã„è¨ˆæ¸¬ã«ã¯ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®CLIPã‚’ç‰¹å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šãã†ã§ã™ã€‚

ãã®ä»£ã‚ã‚Šã€Stable Diffusionãƒ¢ãƒ‡ãƒ«ã‹ã‚‰CLIPã‚’å–ã‚Šå‡ºã—ã¦ä½¿ã†ã“ã¨ãŒã§ãã‚‹ã®ã‹ã‚’æ¤œè¨¼ã—ã¾ã—ãŸã€‚

## CLIPã¨ã¯ï¼ˆãŠã•ã‚‰ã„ï¼‰

CLIPã¯ã€ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®ãƒšã‚¢ã‚’ç”¨ã„ã¦ã€Textã®Encoderã¨ç”»åƒã®EncoderãŒè¿‘ã„åŸ‹ã‚è¾¼ã¿ã‚’è¿”ã™ã‚ˆã†ã«è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚

![CLIP](https://github.com/openai/CLIP/blob/main/CLIP.png?raw=true)

CLIPã®ã†ã¡ã€Stable Diffusionã«åŸ‹ã‚è¾¼ã¾ã‚Œã¦ã„ã‚‹ã®ã¯ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆä¸­ã«ãƒ”ãƒ³ã‚¯ã§ç¤ºã—ãŸéƒ¨åˆ†ã§ã™ã€‚

ï¼ˆViT/L-14ã‚’ã‚‚ã¨ã«ä½œå›³ã€‚å‹‰å¼·ä¸­ã®ãŸã‚èª¤ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰

```mermaid
flowchart TB

text_input([ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›<br/>A photo of a dog]):::simple
image_input([ç”»åƒå…¥åŠ›<br/>çŠ¬ã®ç”»åƒ]):::simple

subgraph text_encoder["Text Encoder"]
direction LR
token_embedding[Token Embedding<br/>49408 x 768]:::sd
positional_embedding[Positional Embedding<br/>77 x 768]:::sd
resblocks_text[Transformer Blocks<br/>12å±¤]:::sd
eos_token(["[CLS] Tokenã®æŠ½å‡º<br/>1 x 768"]):::simple
text_projection[Text Projection<br/>768 x 768]:::simple
text_input --> token_embedding
token_embedding --> positional_embedding
positional_embedding --> resblocks_text --> eos_token --> text_projection
end

subgraph vision_encoder["Vision Encoder"]
direction LR
class_embedding[Class Embedding<br/>1024]:::simple
positional_embedding_vision[Positional Embedding<br/>257 x 1024]:::simple
resblocks_vision[Transformer Blocks<br/>23å±¤]:::simple
cls_token(["[CLS] Tokenã®æŠ½å‡º<br/>1 x 768"]):::simple
visual_projection[Visual Projection<br/>1024 x 768]:::simple
image_input --> class_embedding
class_embedding --> positional_embedding_vision
positional_embedding_vision --> resblocks_vision --> cls_token --> visual_projection
end

output([åŸ‹ã‚è¾¼ã¿è¡¨ç¾<br/>768æ¬¡å…ƒ]):::simple
text_projection --> output
visual_projection --> output

classDef textEncoder fill:#DFD5E6,stroke:#B9A6C5
class text_encoder textEncoder
classDef visionEncoder fill:#D9E7D6,stroke:#B4CDA3
class vision_encoder visionEncoder
classDef simple fill:#fff,stroke:#000
classDef sd fill:#f9f
```

å›³ã®é€šã‚Šã€Stable Diffusionã¯CLIPã®Text Encoderã‚’ã™ã¹ã¦ä½¿ã£ã¦ã„ã‚‹ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚CLIPã¯ãƒ†ã‚­ã‚¹ãƒˆã®æƒ…å ±ã‚’åˆ†é¡ã®ãŸã‚ã®[CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã«é›†ç´„ã•ã›ã¦ä½¿ã£ã¦ã„ã¾ã™ã€‚ä¸€æ–¹ã§ã€Stable Diffusionã§ã¯ç²¾åº¦ã®é«˜ã„æ¨è«–ã®ãŸã‚ã«ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã™ã‚‹åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã‚’ç”¨ã„ã¦ã„ã¾ã™ã€‚

> The stable diffusion model takes both a latent seed and a text prompt as an input. The latent seed is then used to generate random latent image representations of size 64Ã—64 where as the text prompt is transformed to text embeddings of size 77Ã—768 via CLIP's text encoder.[^huggingface_stable_diffusion]

[^huggingface_stable_diffusion]: <https://huggingface.co/blog/stable_diffusion>

å¼•ç”¨ã«ã‚ã‚‹**text embeddings of size 77Ã—768**ã¨ã¯ã€Transformer Blockã‚’çµŒç”±ã—ãŸçŠ¶æ…‹ï¼ˆ[CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®æŠ½å‡ºç›´å‰ï¼‰ã§ã™ã€‚ãªã®ã§ã€CLIPã¨ã—ã¦ã®å‹•ä½œã«å¿…è¦ãªText Projectionå±¤ã¯åŒæ¢±ã•ã‚Œã¦ã„ãªã„ã‚“ã§ã™ã­ã€‚

ã—ãŸãŒã£ã¦ã€å®Ÿã¯Stable Diffusionã‹ã‚‰ç´”ç²‹ãªCLIPã®Text Encoderã‚’å–ã‚Šå‡ºã—ã¦ä½¿ã†ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚

## å®Ÿè£…æ–¹é‡

ã¯ã˜ã‚ã«CLIPã®å®Ÿè£…ã§ã™ãŒã€OpenAI CLIPã‚’ç”¨ã„ã¾ã—ãŸã€‚ä»Šã‹ã‚‰è€ƒãˆã‚‹ã¨ã€[Huggingface Transformersã®CLIPã®CLIPTextModel](https://huggingface.co/docs/transformers/en/model_doc/clip#transformers.CLIPTextModel)ã‚’ä½¿ã£ãŸã»ã†ãŒæ¥½ã ã£ãŸã‹ã‚‚ã—ã‚Œã¾ã›ã‚“...

OpenAI CLIPã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã®é‡ã¿ã®ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹[Issue](https://github.com/openai/CLIP/issues/113)ãŒã‚ã‚Šã€ã“ã‚Œã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚ãŸã ã—ã€Issueã¨ã¯ç•°ãªã‚Šãƒ©ã‚¤ãƒ–ãƒ©ãƒªå´ã®ã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ã—ãªãã¦ã‚‚å‹•ãã‚ˆã†ã«ã—ã¾ã—ãŸã€‚

ã¾ãŸã€å‰è¿°ã®ã¨ãŠã‚ŠStable Diffusionãƒ¢ãƒ‡ãƒ«ã¯CLIPã®Text Projectionã«ã‹ã‹ã‚‹é‡ã¿ã‚’ã‚‚ã£ã¦ã„ã¾ã›ã‚“ã€‚ã“ã“ã¯ä»•æ–¹ãªãã€è¨“ç·´æ¸ˆã¿CLIPã‹ã‚‰å–ã£ã¦ãã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚

## å®Ÿè£…

ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’GitHubã«å…¬é–‹ã—ã¾ã—ãŸã€‚

<!-- markdownlint-disable -->
https://github.com/xhiroga/til/blob/86afdf3aa5fffec038ecd40923775ee5a4138d58/software-engineering/openai/clip/_src/text-encoder-only/sd.py

## è©•ä¾¡

ï¼ˆæ¯”è¼ƒçš„æ§‹é€ ãŒã‚·ãƒ³ãƒ—ãƒ«ãªï¼‰Stable Diffusionãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å–ã‚Šå‡ºã—ãŸCLIPã¨ViT/L-14ã§ã€åŒã˜ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã«ã—ã¦ãã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’æ¸¬ã‚Šã¾ã—ãŸã€‚çµæœã€éå¸¸ã«è¿‘ã„å€¤ã«ãªã‚Šã¾ã—ãŸã€‚

```shell
$ uv run python cos_sim.py
texts=['a diagram', 'a dog', 'a cat']
text_features_vitl14.shape=torch.Size([3, 768])
text_features.shape=torch.Size([3, 768])
text_features_sd_clip.shape=torch.Size([3, 768])
similarity=0.9999422497250121, distance=5.775027498788887e-05
```

ãªãœå®Œå…¨ä¸€è‡´ã§ã¯ãªã„ã‚“ã ã‚ã†ï¼Ÿã¨è¬ã¯æ®‹ã‚‹ã‚‚ã®ã®ã€Stable Diffusion1.5ã§ViT/L-14ã‚’ä½¿ã£ã¦ã„ã‚‹ã“ã¨ãŒä½“æ„Ÿã§ãã€è‰¯ã‹ã£ãŸã§ã™ã€‚

## ã¾ã¨ã‚

CLIPãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’Stable Diffusionãƒ¢ãƒ‡ãƒ«ã®CLIPã®é‡ã¿ã§å·®ã—æ›¿ãˆã¦ã‚‚å‹•ä½œã™ã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã—ãŸã€‚
